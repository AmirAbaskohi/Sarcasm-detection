{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from random import shuffle\n",
        "from statistics import mean\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "a7H3Md224PV3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/iSarcasm/train.En.csv\")[[\"tweet\", \"sarcastic\"]]\n",
        "\n",
        "dataset = dataset.dropna(axis = 0)\n",
        "dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dataset.info()\n",
        "print(dataset.iloc[1062])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9nlVsly4mpx",
        "outputId": "12d8cc63-ea98-4e56-fdf8-f80e83f2d2e9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3467 entries, 0 to 3466\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   tweet      3467 non-null   object\n",
            " 1   sarcastic  3467 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 54.3+ KB\n",
            "tweet        Vaccine dose 1. Thank you, science.\n",
            "sarcastic                                      0\n",
            "Name: 1062, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.sarcastic.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGTUx2434tvB",
        "outputId": "6736cdc1-caca-4651-a26d-2b1393b6027f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2600\n",
              "1     867\n",
              "Name: sarcastic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7pPuvC904uvp",
        "outputId": "cd3a2b77-0f57-43a4-e707-29ad28d5d24d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  sarcastic\n",
              "0  The only thing I got from college is a caffein...          1\n",
              "1  I love it when professors draw a big question ...          1\n",
              "2  Remember the hundred emails from companies whe...          1\n",
              "3  Today my pop-pop told me I was not “forced” to...          1\n",
              "4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = dataset.tweet\n",
        "Y_data = dataset.sarcastic"
      ],
      "metadata": {
        "id": "0R31qpb5-8sh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 150\n",
        "trunc_type = 'post'\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "sequences = tokenizer.texts_to_sequences(X_data)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "X = padded\n",
        "Y = Y_data"
      ],
      "metadata": {
        "id": "krlUJjFA-_TX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlXDTYkvSiiC",
        "outputId": "ceff28f9-7d74-4a05-d1d0-24ca10cf6da5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_24 (Embedding)    (None, 150, 16)           160000    \n",
            "                                                                 \n",
            " bidirectional_24 (Bidirecti  (None, 64)               12544     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 6)                 390       \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 172,941\n",
            "Trainable params: 172,941\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  class_weights = {1:3, 0:1}\n",
        "  train = train.tolist()\n",
        "  test = test.tolist()\n",
        "  shuffle(test)\n",
        "  shuffle(train)\n",
        "\n",
        "  model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', f1_m])\n",
        "  \n",
        "  history = model_lstm.fit(X[train], Y[train], batch_size=32, epochs=5, validation_data=(X[test], Y[test]), class_weight=class_weights,shuffle=True)\n",
        "  \n",
        "  fold_no = fold_no + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2CfjHTN_bbr",
        "outputId": "9b2b4e38-e463-4a9a-8adf-12e11a68393a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 69ms/step - loss: 1.0400 - accuracy: 0.5513 - f1_m: 0.1667 - val_loss: 0.6916 - val_accuracy: 0.7493 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0025 - accuracy: 0.5984 - f1_m: 0.4242 - val_loss: 0.6702 - val_accuracy: 0.5793 - val_f1_m: 0.3962\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.7069 - accuracy: 0.8042 - f1_m: 0.6692 - val_loss: 0.7198 - val_accuracy: 0.5965 - val_f1_m: 0.3759\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.3278 - accuracy: 0.9199 - f1_m: 0.8465 - val_loss: 0.9214 - val_accuracy: 0.5965 - val_f1_m: 0.3450\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.1454 - accuracy: 0.9673 - f1_m: 0.9371 - val_loss: 0.8886 - val_accuracy: 0.6340 - val_f1_m: 0.3457\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 68ms/step - loss: 1.0382 - accuracy: 0.6000 - f1_m: 0.1083 - val_loss: 0.6921 - val_accuracy: 0.7378 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0380 - accuracy: 0.5096 - f1_m: 0.1730 - val_loss: 0.6921 - val_accuracy: 0.7378 - val_f1_m: 0.0000e+00\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0380 - accuracy: 0.7513 - f1_m: 0.0000e+00 - val_loss: 0.6922 - val_accuracy: 0.7378 - val_f1_m: 0.0000e+00\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 1.0380 - accuracy: 0.7513 - f1_m: 0.0000e+00 - val_loss: 0.6917 - val_accuracy: 0.7378 - val_f1_m: 0.0000e+00\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0380 - accuracy: 0.7513 - f1_m: 0.0000e+00 - val_loss: 0.6918 - val_accuracy: 0.7378 - val_f1_m: 0.0000e+00\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 69ms/step - loss: 1.0335 - accuracy: 0.5003 - f1_m: 0.2526 - val_loss: 0.6840 - val_accuracy: 0.7118 - val_f1_m: 0.0347\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.9576 - accuracy: 0.7269 - f1_m: 0.4212 - val_loss: 0.6320 - val_accuracy: 0.6455 - val_f1_m: 0.4591\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.5512 - accuracy: 0.8587 - f1_m: 0.7370 - val_loss: 0.6708 - val_accuracy: 0.6657 - val_f1_m: 0.3863\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.2232 - accuracy: 0.9564 - f1_m: 0.9058 - val_loss: 0.9434 - val_accuracy: 0.6628 - val_f1_m: 0.3834\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.0879 - accuracy: 0.9840 - f1_m: 0.9648 - val_loss: 1.1131 - val_accuracy: 0.6455 - val_f1_m: 0.4264\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 76ms/step - loss: 1.0451 - accuracy: 0.2603 - f1_m: 0.3913 - val_loss: 0.6944 - val_accuracy: 0.2161 - val_f1_m: 0.3464\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0451 - accuracy: 0.2538 - f1_m: 0.3989 - val_loss: 0.6952 - val_accuracy: 0.2161 - val_f1_m: 0.3464\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 1.0450 - accuracy: 0.2538 - f1_m: 0.3988 - val_loss: 0.6966 - val_accuracy: 0.2161 - val_f1_m: 0.3464\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0451 - accuracy: 0.2538 - f1_m: 0.3982 - val_loss: 0.6974 - val_accuracy: 0.2161 - val_f1_m: 0.3464\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0450 - accuracy: 0.2538 - f1_m: 0.4008 - val_loss: 0.6970 - val_accuracy: 0.2161 - val_f1_m: 0.3464\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0380 - accuracy: 0.5734 - f1_m: 0.1358 - val_loss: 0.6909 - val_accuracy: 0.7320 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0351 - accuracy: 0.7599 - f1_m: 0.1353 - val_loss: 0.6569 - val_accuracy: 0.7291 - val_f1_m: 0.1528\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.8174 - accuracy: 0.7651 - f1_m: 0.6175 - val_loss: 0.6867 - val_accuracy: 0.5937 - val_f1_m: 0.4078\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.4400 - accuracy: 0.8962 - f1_m: 0.8039 - val_loss: 0.8011 - val_accuracy: 0.6484 - val_f1_m: 0.3688\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.2022 - accuracy: 0.9548 - f1_m: 0.9119 - val_loss: 0.9294 - val_accuracy: 0.6599 - val_f1_m: 0.4239\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0442 - accuracy: 0.5064 - f1_m: 0.2251 - val_loss: 0.6897 - val_accuracy: 0.6340 - val_f1_m: 0.3015\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.9725 - accuracy: 0.7147 - f1_m: 0.4877 - val_loss: 0.7622 - val_accuracy: 0.4611 - val_f1_m: 0.3631\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.5945 - accuracy: 0.8356 - f1_m: 0.7164 - val_loss: 0.8156 - val_accuracy: 0.5850 - val_f1_m: 0.3021\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.2205 - accuracy: 0.9484 - f1_m: 0.9023 - val_loss: 0.9313 - val_accuracy: 0.6715 - val_f1_m: 0.2186\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.1240 - accuracy: 0.9779 - f1_m: 0.9534 - val_loss: 1.1721 - val_accuracy: 0.6542 - val_f1_m: 0.2283\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0378 - accuracy: 0.3167 - f1_m: 0.3802 - val_loss: 0.6939 - val_accuracy: 0.4092 - val_f1_m: 0.4140\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0397 - accuracy: 0.4990 - f1_m: 0.4694 - val_loss: 0.6980 - val_accuracy: 0.4899 - val_f1_m: 0.4177\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.7434 - accuracy: 0.8179 - f1_m: 0.6627 - val_loss: 0.7251 - val_accuracy: 0.5937 - val_f1_m: 0.3819\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.3213 - accuracy: 0.9394 - f1_m: 0.8737 - val_loss: 0.8140 - val_accuracy: 0.6599 - val_f1_m: 0.3023\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.1348 - accuracy: 0.9792 - f1_m: 0.9539 - val_loss: 1.0634 - val_accuracy: 0.6023 - val_f1_m: 0.3391\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0416 - accuracy: 0.7305 - f1_m: 0.0163 - val_loss: 0.6928 - val_accuracy: 0.7601 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 60ms/step - loss: 1.0413 - accuracy: 0.4889 - f1_m: 0.2346 - val_loss: 0.6897 - val_accuracy: 0.7601 - val_f1_m: 0.0000e+00\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0350 - accuracy: 0.7331 - f1_m: 0.1831 - val_loss: 0.6890 - val_accuracy: 0.5087 - val_f1_m: 0.3865\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.7373 - accuracy: 0.8094 - f1_m: 0.6663 - val_loss: 0.7228 - val_accuracy: 0.5636 - val_f1_m: 0.3674\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.3285 - accuracy: 0.9228 - f1_m: 0.8515 - val_loss: 0.8907 - val_accuracy: 0.5780 - val_f1_m: 0.3556\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0360 - accuracy: 0.5425 - f1_m: 0.1682 - val_loss: 0.6841 - val_accuracy: 0.7225 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 60ms/step - loss: 1.0090 - accuracy: 0.6905 - f1_m: 0.3696 - val_loss: 0.6451 - val_accuracy: 0.6647 - val_f1_m: 0.3536\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.7919 - accuracy: 0.7687 - f1_m: 0.6066 - val_loss: 0.6406 - val_accuracy: 0.6618 - val_f1_m: 0.3835\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.3907 - accuracy: 0.9141 - f1_m: 0.8318 - val_loss: 0.9541 - val_accuracy: 0.5723 - val_f1_m: 0.4234\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.1602 - accuracy: 0.9622 - f1_m: 0.9196 - val_loss: 0.9774 - val_accuracy: 0.6214 - val_f1_m: 0.4288\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "98/98 [==============================] - 11s 70ms/step - loss: 1.0465 - accuracy: 0.2566 - f1_m: 0.3986 - val_loss: 0.7001 - val_accuracy: 0.2110 - val_f1_m: 0.3417\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 1.0354 - accuracy: 0.3159 - f1_m: 0.4143 - val_loss: 0.7150 - val_accuracy: 0.3468 - val_f1_m: 0.3660\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 6s 60ms/step - loss: 0.9224 - accuracy: 0.5902 - f1_m: 0.5413 - val_loss: 0.8183 - val_accuracy: 0.5462 - val_f1_m: 0.3683\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.9901 - accuracy: 0.7840 - f1_m: 0.6959 - val_loss: 0.7302 - val_accuracy: 0.6705 - val_f1_m: 0.3563\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.6275 - accuracy: 0.9045 - f1_m: 0.8106 - val_loss: 0.7769 - val_accuracy: 0.6561 - val_f1_m: 0.3654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZJQbl5PTAF2",
        "outputId": "4d8e43df-cb9b-4c1d-ace6-dc0254feeda3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.2566485106945038,\n",
              "  0.31592437624931335,\n",
              "  0.5901954770088196,\n",
              "  0.784043550491333,\n",
              "  0.9045177698135376],\n",
              " 'f1_m': [0.3986417353153229,\n",
              "  0.41430947184562683,\n",
              "  0.5412946939468384,\n",
              "  0.6958963871002197,\n",
              "  0.8106317520141602],\n",
              " 'loss': [1.0464783906936646,\n",
              "  1.035431981086731,\n",
              "  0.9223549365997314,\n",
              "  0.9901016354560852,\n",
              "  0.6275005340576172],\n",
              " 'val_accuracy': [0.21098266541957855,\n",
              "  0.34682080149650574,\n",
              "  0.5462427735328674,\n",
              "  0.6705202460289001,\n",
              "  0.6560693383216858],\n",
              " 'val_f1_m': [0.3416599929332733,\n",
              "  0.3660137355327606,\n",
              "  0.36833885312080383,\n",
              "  0.35634878277778625,\n",
              "  0.36536893248558044],\n",
              " 'val_loss': [0.7001060843467712,\n",
              "  0.7149655818939209,\n",
              "  0.8182501196861267,\n",
              "  0.730193555355072,\n",
              "  0.7769067287445068]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}