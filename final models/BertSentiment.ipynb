{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3373701d",
   "metadata": {
    "id": "3373701d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546bc28",
   "metadata": {
    "id": "9546bc28"
   },
   "source": [
    "### Import required transformer libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8G-zJPb71y64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G-zJPb71y64",
    "outputId": "68f0fe05-04b4-4851-fa57-828bc8ec3f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 7.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 40.3 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 49.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 503 kB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 41.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a356a10",
   "metadata": {
    "id": "9a356a10"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizerFast,DistilBertForSequenceClassification\n",
    "from transformers import Trainer,TrainingArguments\n",
    "from transformers import DistilBertTokenizerFast, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2deebf5",
   "metadata": {
    "id": "f2deebf5"
   },
   "source": [
    "### Some needed liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba64ac5",
   "metadata": {
    "id": "8ba64ac5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from google.colab import drive\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "# from dataset import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f97dc",
   "metadata": {
    "id": "f69f97dc"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c326b4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c326b4c",
    "outputId": "6d30abe9-5791-47b3-96f7-7b87e2ca2378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# path = 'drive/My Drive/DataLab/sarcasm/train.En.csv'\n",
    "path = 'drive/My Drive/DataLab/sarcasm/train.En.csv'\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv(path)\n",
    "df = df.dropna(subset=['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a426b4",
   "metadata": {
    "id": "25a426b4"
   },
   "source": [
    "## Split Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e4fa16",
   "metadata": {
    "id": "72e4fa16"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e73737d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "4e73737d",
    "outputId": "cc7ee1da-c58d-4c90-db8a-c1d5e9d6a238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8a632bf0-416f-4f5a-869a-ff7426d8b619\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>281</td>\n",
       "      <td>Mental shoot out ends. Silent pause. Sirens st...</td>\n",
       "      <td>1</td>\n",
       "      <td>The sirens always start just as the gunfire fi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>256</td>\n",
       "      <td>did you know there is a direct correlation wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cuffing your pants too high is embarrassing an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>3126</td>\n",
       "      <td>Christmas sucked. Didn't get the heelys I want...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>@mjborshell @garygilligan Hasn't he opened a f...</td>\n",
       "      <td>1</td>\n",
       "      <td>Javid hasn't actually opened any new hospitals...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>1885</td>\n",
       "      <td>omg text “pew pew” and see what happens 😮</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a632bf0-416f-4f5a-869a-ff7426d8b619')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8a632bf0-416f-4f5a-869a-ff7426d8b619 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8a632bf0-416f-4f5a-869a-ff7426d8b619');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0  ... rhetorical_question\n",
       "281          281  ...                 0.0\n",
       "256          256  ...                 1.0\n",
       "3126        3126  ...                 NaN\n",
       "134          134  ...                 0.0\n",
       "1885        1885  ...                 NaN\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb581e05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "bb581e05",
    "outputId": "6a9be4f8-5cfb-47b4-e7ef-082d37f30395"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c96d15de-bd52-4e8f-89b6-48d1968daf7b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>1735</td>\n",
       "      <td>I finished my MBio in Biomedical science this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>366</td>\n",
       "      <td>If your website still has a google plus share ...</td>\n",
       "      <td>1</td>\n",
       "      <td>You should remove your google plus share butto...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Not at all concerning that a man's just been r...</td>\n",
       "      <td>1</td>\n",
       "      <td>Very concerning that a man's just been round t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>2692</td>\n",
       "      <td>Feel like pure shit just want my @depop accoun...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1253</td>\n",
       "      <td>Good morning to everyone except Tristan Thomps...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c96d15de-bd52-4e8f-89b6-48d1968daf7b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c96d15de-bd52-4e8f-89b6-48d1968daf7b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c96d15de-bd52-4e8f-89b6-48d1968daf7b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0  ... rhetorical_question\n",
       "1735        1735  ...                 NaN\n",
       "366          366  ...                 0.0\n",
       "27            27  ...                 0.0\n",
       "2692        2692  ...                 NaN\n",
       "1253        1253  ...                 NaN\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba42fb6",
   "metadata": {
    "id": "6ba42fb6"
   },
   "source": [
    "## Extract Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab8bc05",
   "metadata": {
    "id": "9ab8bc05"
   },
   "outputs": [],
   "source": [
    "train_tweets = train['tweet'].values.tolist()\n",
    "train_labels = train['sarcastic'].values.tolist()\n",
    "test_tweets = test['tweet'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d6989",
   "metadata": {
    "id": "af0d6989"
   },
   "source": [
    "## Split the training sample into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef1160f4",
   "metadata": {
    "id": "ef1160f4"
   },
   "outputs": [],
   "source": [
    "train_tweets, val_tweets, train_labels, val_labels = train_test_split(train_tweets, train_labels, \n",
    "                                                                    test_size=0.1,random_state=42,stratify=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002d240",
   "metadata": {
    "id": "1002d240"
   },
   "source": [
    "## Steps for Fine Tuning model\n",
    "\n",
    "<ul>\n",
    "<li>Prepare dataset</li>\n",
    "<li>Load pretrained tokenizer,call it with dataset</li>\n",
    "<li>Build Pytorch datasets with encodings</li>\n",
    "<li>Load pretrained Model</li>\n",
    "<li> Load Trainer and train it </li>\n",
    "    Instead of Trainer we could've use native Pytorch training pipline.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f18ab",
   "metadata": {
    "id": "af2f18ab"
   },
   "source": [
    "### Set Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e4f36b",
   "metadata": {
    "id": "94e4f36b"
   },
   "outputs": [],
   "source": [
    "model_name = 'detecting-sarcasim'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5276b",
   "metadata": {
    "id": "23f5276b"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "689572b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "689572b4",
    "outputId": "620b58b5-5927-4f25-eeff-bd1a67e09059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-cased',\n",
    "                                                    num_labels=2,\n",
    "                                                    loss_function_params={\"weight\": [0.1, 0.3]}\n",
    "                                                    )\n",
    "\n",
    "# number of labels here is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ee288",
   "metadata": {
    "id": "8f2ee288"
   },
   "source": [
    "<blockquote> The difference between a “fast” and a “non-fast” tokenizer is computation speed but there is no functional difference between them.\n",
    "<blockqoute> FastTokenizers are implemented in Rust and are factors faster than the Python based tokenizers. Apart from that their encoding methods should behave the same. However, they are not functionally identical.</blockqoute>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b73eded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b73eded",
    "outputId": "6107911d-8957-4246-c5db-abdb57c909cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_tweets, truncation=True, padding=True,return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2af2b52",
   "metadata": {
    "id": "f2af2b52"
   },
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(val_tweets, truncation=True, padding=True,return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cac5e7f3",
   "metadata": {
    "id": "cac5e7f3"
   },
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_tweets, truncation=True, padding=True,return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d6992",
   "metadata": {
    "id": "a41d6992"
   },
   "source": [
    "<ul>\n",
    "    <li>setting truncation = True will eliminate tokens that exceed the max_length(512) in case of BERT.</li>\n",
    "    <li>setting padding =True will pad documents that have length less than max_length with empty tokens i.e. 0, ensuring that all of our sequences are padded to the same length.</li>\n",
    "    <li>setting return_tensors = ‘pt’ will return the encodings as pytorch tensors.</li>\n",
    "    <li>This will allow us to feed batches of sequences into the model at the same time.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566c242",
   "metadata": {
    "id": "6566c242"
   },
   "source": [
    "## Turn labels and encodings into a Dataset object\n",
    "\n",
    "<ul>\n",
    "    <li>Wrap the tokenized data into a torch dataset.</li>\n",
    "    <li>In PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing len and getitem.</li>\n",
    "<ul>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f61f1b62",
   "metadata": {
    "id": "f61f1b62"
   },
   "outputs": [],
   "source": [
    "class SarcasimDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "## Test Dataset\n",
    "class SarcasimTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b680849",
   "metadata": {
    "id": "0b680849"
   },
   "source": [
    "## Genearte DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e85c07c5",
   "metadata": {
    "id": "e85c07c5"
   },
   "outputs": [],
   "source": [
    "train_dataset = SarcasimDataset(train_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba396d41",
   "metadata": {
    "id": "ba396d41"
   },
   "outputs": [],
   "source": [
    "val_dataset = SarcasimDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a320683",
   "metadata": {
    "id": "1a320683"
   },
   "outputs": [],
   "source": [
    "test_dataset = SarcasimTestDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a038b",
   "metadata": {
    "id": "9d2a038b"
   },
   "source": [
    "## Define a Simple Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1699f8cc",
   "metadata": {
    "id": "1699f8cc"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    #recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    #precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(labels, pred, average='weighted')\n",
    "\n",
    "    return {\"accuracy\": accuracy,\"f1_score\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aef78ffb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aef78ffb",
    "outputId": "662fecec-158e-4a14-b551-3f5bda3e01e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./res', evaluation_strategy=\"steps\", num_train_epochs=5, per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64, warmup_steps=500, weight_decay=0.01,logging_dir='./logs4',\n",
    "    #logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44fd09",
   "metadata": {
    "id": "5f44fd09"
   },
   "source": [
    "<ul>\n",
    "    <li> output_dir = output directory</li>\n",
    "    <li> num_train_epochs = total number of training epochs</li>\n",
    "    <li> per_device_train_batch_size = batch size per device during training</li>\n",
    "    <li> per_device_eval_batch_size = batch size for evaluation</li>\n",
    "    <li> warmup_steps = number of warmup steps for learning rate scheduler</li>\n",
    "    <li> weight_decay = strength of weight decay</li>\n",
    "    <li> logging_dir = directory for storing logs</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4cc62",
   "metadata": {
    "id": "72e4cc62"
   },
   "source": [
    "## Fine Tuning with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8836857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8836857",
    "outputId": "fb5b0428-3b55-40ea-da72-9a32e002b406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.10.sa_layer_norm.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.3.attention.k_lin.weight', 'pre_classifier.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'classifier.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'classifier.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'pre_classifier.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.4.attention.k_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=2)\n",
    "# distilbert-base-uncased\n",
    "# bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "OHEt5s5ku0mS",
   "metadata": {
    "id": "OHEt5s5ku0mS"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.3]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "164e5fab",
   "metadata": {
    "id": "164e5fab"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6975796",
   "metadata": {
    "id": "f6975796"
   },
   "source": [
    "<ul>\n",
    "    <li> model = the instantiated hugging-face Transformers model to be trained </li>\n",
    "    <li> args=training_args = training arguments, defined above</li>\n",
    "    <li> train_dataset = training dataset</li>\n",
    "    <li> eval_dataset = evaluation dataset</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2a410128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "2a410128",
    "outputId": "d3f5363a-5113-4abf-a0b1-b5da5fa24775"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2808\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 02:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=440, training_loss=0.5536378340287642, metrics={'train_runtime': 178.0762, 'train_samples_per_second': 78.843, 'train_steps_per_second': 2.471, 'total_flos': 916304805842400.0, 'train_loss': 0.5536378340287642, 'epoch': 5.0})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0efe28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d7117df8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "d7117df8",
    "outputId": "bc0a9d1c-cdd9-4d47-be33-198cd30808d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 312\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 5.0,\n",
       " 'eval_accuracy': 0.6442307692307693,\n",
       " 'eval_f1_score': 0.6620442509055567,\n",
       " 'eval_loss': 0.6211642026901245,\n",
       " 'eval_runtime': 1.1694,\n",
       " 'eval_samples_per_second': 266.8,\n",
       " 'eval_steps_per_second': 4.276}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "708e22ac",
   "metadata": {
    "id": "708e22ac"
   },
   "outputs": [],
   "source": [
    "# test['sarcastic'] = 0\n",
    "# test_tweets = test['tweet'].values.tolist() \n",
    "# test_labels = test['sarcastic'].values.tolist() \n",
    "# test_encodings = tokenizer(test_tweets,\n",
    "#                            truncation=True, \n",
    "#                            padding=True,\n",
    "#                            return_tensors = 'pt').to(\"cuda\") \n",
    "# test_dataset = SentimentDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvNN1cgkG2eo",
   "metadata": {
    "id": "GvNN1cgkG2eo"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e3a1f5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "2e3a1f5e",
    "outputId": "4b6fe5ac-bfc5-4960-c70c-3963376e35e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pin_memory=False\n",
    "preds = trainer.predict(test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "TODokk_L95K_",
   "metadata": {
    "id": "TODokk_L95K_"
   },
   "outputs": [],
   "source": [
    "probs = torch.from_numpy(preds[0]).softmax(1)\n",
    "\n",
    "# convert tensors to numpy array\n",
    "predictions = probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79wkTirvA74n",
   "metadata": {
    "id": "79wkTirvA74n"
   },
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame(predictions,columns=['Neutral_1','Positive_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "XL9BbP_9YosP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "XL9BbP_9YosP",
    "outputId": "7fd4a78d-84d8-452a-b70e-bd34999e2306"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2691f46c-6ae5-4c86-bb4d-87eabeb0df0e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neutral_1</th>\n",
       "      <th>Positive_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.717921</td>\n",
       "      <td>0.282079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712581</td>\n",
       "      <td>0.287419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2691f46c-6ae5-4c86-bb4d-87eabeb0df0e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2691f46c-6ae5-4c86-bb4d-87eabeb0df0e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2691f46c-6ae5-4c86-bb4d-87eabeb0df0e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Neutral_1  Positive_2\n",
       "0   0.717921    0.282079\n",
       "1   0.712581    0.287419"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "kNyNX9MVA_RX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "kNyNX9MVA_RX",
    "outputId": "ea353ce6-73e6-488f-ce44-f47d082ad8db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f7df0a70-17fc-453e-9072-144e02fb3072\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>1735</td>\n",
       "      <td>I finished my MBio in Biomedical science this ...</td>\n",
       "      <td>Positive_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>366</td>\n",
       "      <td>If your website still has a google plus share ...</td>\n",
       "      <td>Positive_1</td>\n",
       "      <td>You should remove your google plus share butto...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Not at all concerning that a man's just been r...</td>\n",
       "      <td>Positive_1</td>\n",
       "      <td>Very concerning that a man's just been round t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>2692</td>\n",
       "      <td>Feel like pure shit just want my @depop accoun...</td>\n",
       "      <td>Positive_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1253</td>\n",
       "      <td>Good morning to everyone except Tristan Thomps...</td>\n",
       "      <td>Positive_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7df0a70-17fc-453e-9072-144e02fb3072')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f7df0a70-17fc-453e-9072-144e02fb3072 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f7df0a70-17fc-453e-9072-144e02fb3072');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      Unnamed: 0  ... rhetorical_question\n",
       "1735        1735  ...                 NaN\n",
       "366          366  ...                 0.0\n",
       "27            27  ...                 0.0\n",
       "2692        2692  ...                 NaN\n",
       "1253        1253  ...                 NaN\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labels(x):\n",
    "  if x == 0:\n",
    "    return 'Negative_1'\n",
    "  else:\n",
    "    return 'Positive_1'\n",
    "\n",
    "results = np.argmax(predictions,axis=1)\n",
    "# test['sarcastic'] = results\n",
    "test['sarcastic'] = test['sarcastic'].map(labels)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ri-rYHPSYdBM",
   "metadata": {
    "id": "ri-rYHPSYdBM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BertSentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
