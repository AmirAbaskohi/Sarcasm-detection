{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYVqVOj7HGOM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from random import shuffle\n",
        "from statistics import mean\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWtBz6ebeNz5",
        "outputId": "7ab659fe-ae9a-4d84-b945-7fa11254a29d"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/isarcasm/train.En.csv\")[[\"tweet\", \"sarcastic\"]]\n",
        "\n",
        "dataset = dataset.dropna(axis = 0)\n",
        "dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "dataset.info()\n",
        "print(dataset.iloc[1062])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3467 entries, 0 to 3466\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   tweet      3467 non-null   object\n",
            " 1   sarcastic  3467 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 54.3+ KB\n",
            "tweet        Vaccine dose 1. Thank you, science.\n",
            "sarcastic                                      0\n",
            "Name: 1062, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxVUk9oKSHBp",
        "outputId": "1d04efeb-2309-4d89-a0c0-05513d3206a5"
      },
      "source": [
        "dataset.sarcastic.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2600\n",
              "1     867\n",
              "Name: sarcastic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "x1MW6I5HcsZ3",
        "outputId": "fa23ccb7-1219-410a-f45a-4bf74ff17f67"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  sarcastic\n",
              "0  The only thing I got from college is a caffein...          1\n",
              "1  I love it when professors draw a big question ...          1\n",
              "2  Remember the hundred emails from companies whe...          1\n",
              "3  Today my pop-pop told me I was not “forced” to...          1\n",
              "4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4vEhN7idGde"
      },
      "source": [
        "X_data = dataset.tweet\n",
        "Y_data = dataset.sarcastic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtbKuUwAeot_"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 150\n",
        "trunc_type = 'post'\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "sequences = tokenizer.texts_to_sequences(X_data)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "X = padded\n",
        "Y = Y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMXONT4gHA2p"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHpeMe09iBDT",
        "outputId": "2890d0fb-5dda-4d53-b132-c1f1771c7b6c"
      },
      "source": [
        "model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 150, 16)           160000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 32)                4224      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 198       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 164,429\n",
            "Trainable params: 164,429\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqk6P6NNJ5nQ",
        "outputId": "c7ba2732-ac26-4fb1-ccb9-fffc1cc445bf"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  class_weights = {1:2, 0:0.67}\n",
        "  train = train.tolist()\n",
        "  test = test.tolist()\n",
        "  shuffle(test)\n",
        "  shuffle(train)\n",
        "\n",
        "  model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', f1_m])\n",
        "  \n",
        "  history = model_lstm.fit(X[train], Y[train], batch_size=32, epochs=5, validation_data=(X[test], Y[test]), class_weight=class_weights,shuffle=True)\n",
        "  \n",
        "  fold_no = fold_no + 1\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "87/87 [==============================] - 14s 115ms/step - loss: 0.6939 - accuracy: 0.5568 - f1_m: 0.2271 - val_loss: 0.6973 - val_accuracy: 0.3689 - val_f1_m: 0.4131\n",
            "Epoch 2/5\n",
            "87/87 [==============================] - 9s 103ms/step - loss: 0.6485 - accuracy: 0.6221 - f1_m: 0.5038 - val_loss: 0.6691 - val_accuracy: 0.6095 - val_f1_m: 0.3867\n",
            "Epoch 3/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.3682 - accuracy: 0.8583 - f1_m: 0.7393 - val_loss: 0.6658 - val_accuracy: 0.6830 - val_f1_m: 0.3875\n",
            "Epoch 4/5\n",
            "87/87 [==============================] - 9s 103ms/step - loss: 0.1427 - accuracy: 0.9513 - f1_m: 0.9018 - val_loss: 0.8866 - val_accuracy: 0.6225 - val_f1_m: 0.3765\n",
            "Epoch 5/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.0609 - accuracy: 0.9816 - f1_m: 0.9616 - val_loss: 0.9574 - val_accuracy: 0.6931 - val_f1_m: 0.3395\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "87/87 [==============================] - 15s 118ms/step - loss: 0.6938 - accuracy: 0.2506 - f1_m: 0.3890 - val_loss: 0.6934 - val_accuracy: 0.2565 - val_f1_m: 0.4038\n",
            "Epoch 2/5\n",
            "87/87 [==============================] - 9s 106ms/step - loss: 0.6912 - accuracy: 0.3913 - f1_m: 0.3817 - val_loss: 0.7170 - val_accuracy: 0.3012 - val_f1_m: 0.4134\n",
            "Epoch 3/5\n",
            "87/87 [==============================] - 9s 106ms/step - loss: 0.6002 - accuracy: 0.6574 - f1_m: 0.5441 - val_loss: 0.6957 - val_accuracy: 0.5764 - val_f1_m: 0.3624\n",
            "Epoch 4/5\n",
            "87/87 [==============================] - 9s 105ms/step - loss: 0.3124 - accuracy: 0.8983 - f1_m: 0.8035 - val_loss: 0.7494 - val_accuracy: 0.6383 - val_f1_m: 0.3568\n",
            "Epoch 5/5\n",
            "87/87 [==============================] - 9s 107ms/step - loss: 0.1420 - accuracy: 0.9629 - f1_m: 0.9119 - val_loss: 1.0207 - val_accuracy: 0.6455 - val_f1_m: 0.3485\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "87/87 [==============================] - 15s 116ms/step - loss: 0.7010 - accuracy: 0.3183 - f1_m: 0.3641 - val_loss: 0.6898 - val_accuracy: 0.7763 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.6975 - accuracy: 0.7426 - f1_m: 0.1851 - val_loss: 0.6703 - val_accuracy: 0.6797 - val_f1_m: 0.3141\n",
            "Epoch 3/5\n",
            "87/87 [==============================] - 9s 107ms/step - loss: 0.6234 - accuracy: 0.8241 - f1_m: 0.5419 - val_loss: 0.6057 - val_accuracy: 0.6421 - val_f1_m: 0.2966\n",
            "Epoch 4/5\n",
            "87/87 [==============================] - 9s 105ms/step - loss: 0.4597 - accuracy: 0.9156 - f1_m: 0.8176 - val_loss: 0.6217 - val_accuracy: 0.6508 - val_f1_m: 0.3099\n",
            "Epoch 5/5\n",
            "87/87 [==============================] - 9s 106ms/step - loss: 0.3560 - accuracy: 0.9542 - f1_m: 0.9035 - val_loss: 0.7595 - val_accuracy: 0.7013 - val_f1_m: 0.2380\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "87/87 [==============================] - 14s 117ms/step - loss: 0.6943 - accuracy: 0.3792 - f1_m: 0.3253 - val_loss: 0.6890 - val_accuracy: 0.7201 - val_f1_m: 0.1862\n",
            "Epoch 2/5\n",
            "87/87 [==============================] - 9s 103ms/step - loss: 0.6762 - accuracy: 0.6009 - f1_m: 0.4830 - val_loss: 0.6761 - val_accuracy: 0.5657 - val_f1_m: 0.4248\n",
            "Epoch 3/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.4790 - accuracy: 0.8180 - f1_m: 0.6899 - val_loss: 0.7046 - val_accuracy: 0.6147 - val_f1_m: 0.3925\n",
            "Epoch 4/5\n",
            "87/87 [==============================] - 9s 107ms/step - loss: 0.2148 - accuracy: 0.9445 - f1_m: 0.8866 - val_loss: 0.7654 - val_accuracy: 0.6508 - val_f1_m: 0.3218\n",
            "Epoch 5/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.0967 - accuracy: 0.9787 - f1_m: 0.9560 - val_loss: 0.7885 - val_accuracy: 0.6926 - val_f1_m: 0.2569\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "87/87 [==============================] - 14s 115ms/step - loss: 0.6917 - accuracy: 0.7516 - f1_m: 0.0000e+00 - val_loss: 0.6868 - val_accuracy: 0.7359 - val_f1_m: 0.0000e+00\n",
            "Epoch 2/5\n",
            "87/87 [==============================] - 9s 105ms/step - loss: 0.6764 - accuracy: 0.7632 - f1_m: 0.1015 - val_loss: 0.6111 - val_accuracy: 0.7345 - val_f1_m: 0.0993\n",
            "Epoch 3/5\n",
            "87/87 [==============================] - 9s 103ms/step - loss: 0.5451 - accuracy: 0.8500 - f1_m: 0.6408 - val_loss: 0.6341 - val_accuracy: 0.6421 - val_f1_m: 0.3163\n",
            "Epoch 4/5\n",
            "87/87 [==============================] - 9s 104ms/step - loss: 0.2666 - accuracy: 0.9275 - f1_m: 0.8622 - val_loss: 0.8324 - val_accuracy: 0.6003 - val_f1_m: 0.3829\n",
            "Epoch 5/5\n",
            "87/87 [==============================] - 9s 103ms/step - loss: 0.1046 - accuracy: 0.9694 - f1_m: 0.9403 - val_loss: 0.9881 - val_accuracy: 0.6061 - val_f1_m: 0.3288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6vLptLRDtgj",
        "outputId": "48ac49a1-d763-4f28-f466-1ffe43f9aaec"
      },
      "source": [
        "fold_no = 1\n",
        "model_with_weights_f1s = list()\n",
        "\n",
        "for train, test in kfold.split(X):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  train = train.tolist()\n",
        "  test = test.tolist()\n",
        "  shuffle(test)\n",
        "  shuffle(train)\n",
        "\n",
        "  model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "\n",
        "  model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', f1_m])\n",
        "\n",
        "  class_weights = {1:2, 0:0.67}\n",
        "  history = model_lstm.fit(X[train], Y[train], batch_size=32, epochs=10, class_weight=class_weights,shuffle=True)\n",
        "  \n",
        "  scores = model_lstm.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: \\n{model_lstm.metrics_names[0]} of {scores[0]};\\n {model_lstm.metrics_names[1]} of {scores[1]*100}%;\\n{model_lstm.metrics_names[2]} of {scores[2]};')\n",
        "  model_with_weights_f1s.append(scores[2])\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(f'average of folds f1s is : {mean(model_with_weights_f1s)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 91ms/step - loss: 0.6959 - accuracy: 0.7432 - f1_m: 0.0292\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.6790 - accuracy: 0.7674 - f1_m: 0.2010\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.5654 - accuracy: 0.8536 - f1_m: 0.6294\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.4147 - accuracy: 0.9275 - f1_m: 0.8416\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.3308 - accuracy: 0.9607 - f1_m: 0.9186\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2937 - accuracy: 0.9744 - f1_m: 0.9467\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.2702 - accuracy: 0.9784 - f1_m: 0.9587\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2473 - accuracy: 0.9805 - f1_m: 0.9595\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2288 - accuracy: 0.9812 - f1_m: 0.9631\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2131 - accuracy: 0.9834 - f1_m: 0.9657\n",
            "Score for fold 1: \n",
            "loss of 1.2571284770965576;\n",
            " accuracy of 62.39193081855774%;\n",
            "f1_m of 0.29269006848335266;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 93ms/step - loss: 0.6945 - accuracy: 0.4602 - f1_m: 0.2593\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.6757 - accuracy: 0.7364 - f1_m: 0.3102\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 93ms/step - loss: 0.4989 - accuracy: 0.8312 - f1_m: 0.6755\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 94ms/step - loss: 0.2294 - accuracy: 0.9239 - f1_m: 0.8492\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0813 - accuracy: 0.9776 - f1_m: 0.9525\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0370 - accuracy: 0.9910 - f1_m: 0.9825\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 93ms/step - loss: 0.0239 - accuracy: 0.9950 - f1_m: 0.9866\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 94ms/step - loss: 0.0165 - accuracy: 0.9960 - f1_m: 0.9917\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 93ms/step - loss: 0.0158 - accuracy: 0.9971 - f1_m: 0.9945\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0081 - accuracy: 0.9978 - f1_m: 0.9953\n",
            "Score for fold 2: \n",
            "loss of 1.5422310829162598;\n",
            " accuracy of 68.1556224822998%;\n",
            "f1_m of 0.26367107033729553;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 93ms/step - loss: 0.6937 - accuracy: 0.5663 - f1_m: 0.2363\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.6388 - accuracy: 0.6896 - f1_m: 0.5230\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.3669 - accuracy: 0.8594 - f1_m: 0.7456\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.1444 - accuracy: 0.9531 - f1_m: 0.8990\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 94ms/step - loss: 0.0554 - accuracy: 0.9841 - f1_m: 0.9675\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 93ms/step - loss: 0.0356 - accuracy: 0.9917 - f1_m: 0.9836\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 93ms/step - loss: 0.0157 - accuracy: 0.9964 - f1_m: 0.9915\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 95ms/step - loss: 0.0141 - accuracy: 0.9971 - f1_m: 0.9950\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 94ms/step - loss: 0.0098 - accuracy: 0.9986 - f1_m: 0.9973\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0077 - accuracy: 0.9975 - f1_m: 0.9948\n",
            "Score for fold 3: \n",
            "loss of 1.4514930248260498;\n",
            " accuracy of 67.6767647266388%;\n",
            "f1_m of 0.29151976108551025;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 11s 90ms/step - loss: 0.6954 - accuracy: 0.6417 - f1_m: 0.0732\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.6953 - accuracy: 0.2808 - f1_m: 0.3755\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.6828 - accuracy: 0.3464 - f1_m: 0.4238\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.5605 - accuracy: 0.6889 - f1_m: 0.6083\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.4170 - accuracy: 0.8969 - f1_m: 0.8157\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.3393 - accuracy: 0.9546 - f1_m: 0.9072\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2993 - accuracy: 0.9766 - f1_m: 0.9511\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.2796 - accuracy: 0.9831 - f1_m: 0.9654\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.2517 - accuracy: 0.9863 - f1_m: 0.9710\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.2312 - accuracy: 0.9906 - f1_m: 0.9809\n",
            "Score for fold 4: \n",
            "loss of 1.02241051197052;\n",
            " accuracy of 64.50216174125671%;\n",
            "f1_m of 0.3708593547344208;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 11s 91ms/step - loss: 0.6946 - accuracy: 0.4279 - f1_m: 0.3128\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.6802 - accuracy: 0.7022 - f1_m: 0.4009\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.4674 - accuracy: 0.8335 - f1_m: 0.7073\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.2141 - accuracy: 0.9358 - f1_m: 0.8684\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0927 - accuracy: 0.9791 - f1_m: 0.9533\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0427 - accuracy: 0.9895 - f1_m: 0.9772\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0489 - accuracy: 0.9852 - f1_m: 0.9705\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0309 - accuracy: 0.9899 - f1_m: 0.9809\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0200 - accuracy: 0.9950 - f1_m: 0.9880\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0134 - accuracy: 0.9968 - f1_m: 0.9926\n",
            "Score for fold 5: \n",
            "loss of 1.329014539718628;\n",
            " accuracy of 63.34776282310486%;\n",
            "f1_m of 0.2937081456184387;\n",
            "average of folds f1s is : 0.3024896800518036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfFtG36OJpXg",
        "outputId": "49825a3f-f826-416e-cd8b-62baa7d88702"
      },
      "source": [
        "fold_no = 1\n",
        "model_without_weights_f1s = list()\n",
        "\n",
        "for train, test in kfold.split(X):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  train = train.tolist()\n",
        "  test = test.tolist()\n",
        "  shuffle(test)\n",
        "  shuffle(train)\n",
        "\n",
        "  model_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "\n",
        "  model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', f1_m])\n",
        "\n",
        "  history = model_lstm.fit(X[train], Y[train], batch_size=32, epochs=10,shuffle=True)\n",
        "  \n",
        "  scores = model_lstm.evaluate(X[test], Y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: \\n{model_lstm.metrics_names[0]} of {scores[0]};\\n {model_lstm.metrics_names[1]} of {scores[1]*100}%;\\n{model_lstm.metrics_names[2]} of {scores[2]};')\n",
        "  model_without_weights_f1s.append(scores[2])\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(f'average of folds f1s is : {mean(model_without_weights_f1s)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 90ms/step - loss: 0.5953 - accuracy: 0.7479 - f1_m: 0.0000e+00\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.5564 - accuracy: 0.7479 - f1_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.4749 - accuracy: 0.7660 - f1_m: 0.1260\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.2432 - accuracy: 0.9228 - f1_m: 0.8272\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.1105 - accuracy: 0.9686 - f1_m: 0.9338\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0578 - accuracy: 0.9852 - f1_m: 0.9666\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0296 - accuracy: 0.9931 - f1_m: 0.9858\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0171 - accuracy: 0.9968 - f1_m: 0.9934\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0141 - accuracy: 0.9975 - f1_m: 0.9949\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0139 - accuracy: 0.9971 - f1_m: 0.9937\n",
            "Score for fold 1: \n",
            "loss of 1.2235075235366821;\n",
            " accuracy of 69.30835843086243%;\n",
            "f1_m of 0.2975965440273285;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 91ms/step - loss: 0.6065 - accuracy: 0.7324 - f1_m: 0.0160\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.5593 - accuracy: 0.7486 - f1_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.5040 - accuracy: 0.7512 - f1_m: 0.0165\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.3018 - accuracy: 0.8821 - f1_m: 0.7013\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.1288 - accuracy: 0.9650 - f1_m: 0.9230\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0641 - accuracy: 0.9827 - f1_m: 0.9612\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0411 - accuracy: 0.9903 - f1_m: 0.9772\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0250 - accuracy: 0.9946 - f1_m: 0.9877\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0168 - accuracy: 0.9968 - f1_m: 0.9912\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0138 - accuracy: 0.9964 - f1_m: 0.9933\n",
            "Score for fold 2: \n",
            "loss of 1.2992018461227417;\n",
            " accuracy of 64.69740867614746%;\n",
            "f1_m of 0.2606031596660614;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 11s 89ms/step - loss: 0.6130 - accuracy: 0.7329 - f1_m: 0.0079\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.5572 - accuracy: 0.7473 - f1_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.4949 - accuracy: 0.7473 - f1_m: 0.0000e+00\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.3098 - accuracy: 0.8432 - f1_m: 0.4876\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.1276 - accuracy: 0.9632 - f1_m: 0.9235\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0450 - accuracy: 0.9895 - f1_m: 0.9781\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0248 - accuracy: 0.9939 - f1_m: 0.9877\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 89ms/step - loss: 0.0177 - accuracy: 0.9968 - f1_m: 0.9931\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0157 - accuracy: 0.9968 - f1_m: 0.9929\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0146 - accuracy: 0.9971 - f1_m: 0.9934\n",
            "Score for fold 3: \n",
            "loss of 1.3538717031478882;\n",
            " accuracy of 69.84127163887024%;\n",
            "f1_m of 0.24598611891269684;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 90ms/step - loss: 0.5875 - accuracy: 0.7502 - f1_m: 0.0084\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.5496 - accuracy: 0.7545 - f1_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.4509 - accuracy: 0.7700 - f1_m: 0.1251\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.2388 - accuracy: 0.9110 - f1_m: 0.7875\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0995 - accuracy: 0.9683 - f1_m: 0.9213\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0427 - accuracy: 0.9892 - f1_m: 0.9750\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0198 - accuracy: 0.9953 - f1_m: 0.9914\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0133 - accuracy: 0.9975 - f1_m: 0.9929\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0102 - accuracy: 0.9975 - f1_m: 0.9955\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 89ms/step - loss: 0.0088 - accuracy: 0.9982 - f1_m: 0.9965\n",
            "Score for fold 4: \n",
            "loss of 1.550926923751831;\n",
            " accuracy of 66.66666865348816%;\n",
            "f1_m of 0.26971060037612915;\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "87/87 [==============================] - 12s 89ms/step - loss: 0.5797 - accuracy: 0.7505 - f1_m: 0.0000e+00\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.5483 - accuracy: 0.7513 - f1_m: 0.0000e+00\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.4683 - accuracy: 0.7635 - f1_m: 0.0850\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.2995 - accuracy: 0.8562 - f1_m: 0.5559\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.1415 - accuracy: 0.9636 - f1_m: 0.9071\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0721 - accuracy: 0.9834 - f1_m: 0.9663\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0466 - accuracy: 0.9895 - f1_m: 0.9779\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 8s 90ms/step - loss: 0.0287 - accuracy: 0.9946 - f1_m: 0.9890\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 8s 91ms/step - loss: 0.0166 - accuracy: 0.9971 - f1_m: 0.9945\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 8s 92ms/step - loss: 0.0149 - accuracy: 0.9971 - f1_m: 0.9943\n",
            "Score for fold 5: \n",
            "loss of 1.2860307693481445;\n",
            " accuracy of 68.10966730117798%;\n",
            "f1_m of 0.2894081771373749;\n",
            "average of folds f1s is : 0.2726609200239182\n"
          ]
        }
      ]
    }
  ]
}